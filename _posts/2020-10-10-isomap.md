---
title: "ISOMAP: Dimensionality Reduction"
---

ISOMAP is a non-linear dimensionality reduction technique mostly used in image classification. Here we will implement our own ISOMAP algorithm and recreate the results from the original ISOMAP paper 

In this post we'll go over:
- Brief recap of PCA (Linear Dimensionality Reduction)
- What ISOMAP is and why its useful
- How to implement the ISOMAP algorithm yourself (by recreating the results from the paper which popularised ISOMAP!)

Before we start, it's recommended you have a basic grasp of linear algebra concepts (namely eigen-decomposition and Euclidean Distance) and Prinicipal Component Analysis as we'll only be covering this breifly

Let's go!

### Principal Component Analysis (..briefly)

PCA is most commonly used to reduce the number of features used in machine learning models while retaining as much useful information as possible. It's not uncommon to have thousands or even millions of features in a model and identifying which are useful can become difficult and computationally expensive.

The new features produced by PCA are called principal components and they are all orthogonal to each other meaning there is 0 correlation between them, let's take a look at a quick example:

In the below we $n=2$ features $x_1$ and  $x_2$.  We use PCA to produce 2 principal components. If we wanted to reduce the number of dimensions (features) to 1 and re-plotted our data using only the PC1 line as the axis, we get a relatively similar representation of our original data. If we plotted using PC2 as the axis we'd lose a lot of information as there is far less variance along this direction
 

![_config.yml]({{ site.baseurl }}/images/isomap/pca1.png)


We create our principal components by estimating the **mean** of our data
$$ \mu = \pmatrix{ \text{mean of } x_1\\ \text{mean of } x_2 \ } = \frac{1}{m} \sum^n_{i=1} x_i = \frac{1}{400} \sum^2_{i=1} x_i$$

And creating a **covariance** matrix, $A$.

$$\hspace{3cm} x_1 \hspace{3cm} x_2 \\ Cov(X) =  \matrix{x_1 \\ x_2}  \pmatrix{ Var(x_1) \hspace{1cm} Cov(x_1, x_2), \\ Cov(x_2, x_1) \hspace{1cm} Var(x_2) }$$ 

Some points to note on the covariance matrix:
- When calculating covariance of data we **center** (subtract the mean from) each data point then compare values. We will be doing the matrix equivalent of "centering" later which is more complex, but just remember it's the equivalent of subracting the mean from each data point!
- Covariance matrix is **symmetrical** ($Cov(x_1, x_2) = Cov(x_2, x_1)$),  and centered which are requirements for eigen-decomposition

We then calculate the **eigenvectors** of the covariance matrix. The eigenvectors are our **principal components**. We won't go into depth of how to calculate these in this post but an eigenvector is a vectors $v$ such that when multiplied by our covariance matrix $A$ the output is a scalar multiple ($\lambda$) of $v$.
$$ Av = \lambda v$$

The scalar values $\lambda$ are called eigenvalues and we rank our eigenvectors by the magnitude of their corresponding $\lambda$. In our example above PC1 would have a larger $\lambda$ than PC2.



### Limitation of PCA

PCA is a **linear** dimensionality reduction technique, **meaning our low dimension representation of the data is calcuated using only linear functions**  and thus we require a linear relationship between our variables.

But.. say we have a non-linear relationship in our data? Below we have 2 clear clusters in our data shaped like a donut, clearly there isn't a linear function we could use to generate meaningful low-dimensional representation of our data

.. this where PCA and other linear dimensionality reduction techniques reach their limit.. in step ISOMAP

![png](../../images/isomap/circles.png)

# ISOMAP

ISOMAP is a non-linear dimensionality reduction technique most commonly used in image classification. We will recreate the results from the original ISOMAP paper ([link here](https://web.mit.edu/cocosci/Papers/sci_reprint.pdf) if you want to take a look!) where we group similar images together.

We have **698 images** ($m=698$) of a face taken from various angles. Some are facing left some are facing right, some from above, some below etc. We'll build our ISOMAP algorithm to reduce the images down to 2 dimensions and plot them, the more similar the image the closer they should be in our 2 dimensional plot. We'll then compare against PCA. Let's go!




```python
import scipy.io
from scipy.spatial.distance import cdist

images = scipy.io.loadmat('data/isomap.mat')['images'].T
```


```python
import matplotlib.pyplot as plt

f,axs = plt.subplots(2,2, figsize=(10,10))
axs = axs.ravel()

for i in range(4):
    axs[i].imshow(images[4+i,:].reshape(64,64).T)
    axs[i].axis('off')
```


![png](../../images/isomap/output_3_0.png)


## Steps

- Create Adjacency Matrix $A$
- Calculate Distance Matrix $D$
- Normalise (center) $D$ to get centered matrix $C$
- Get the first $k$ (in our case 2) eigenvectors of $C$ and plot them

A key difference to between ISOMAP and PCA is how it defines similarity.

As we mentioned, in PCA we use the eigenvectors of the a covariance matrix . In ISOMAP we create a **geodesic matrix** instead of a covariance matrix. A geodesic matrix is a $ m \times m$ matrix of **shortest distances between each point**. Importantly, here distance is the distance we travel **through the data** by "jumping" from point to point, not a stright line distance. The eigenvectors of the geodesic matrix are our principal components.

![png](../../images/isomap/circles2.png)


### Adjacency Matrix

The first step is create an adjacency matrix. First we calculate the distances between all points. Then we identify the $k$ nearest neighbours for each point, we say each point has an **edge** (connection) with each of its $k$-nearest neighbours.  Then we create our $m \times m$ adjacency matrix, $A$ and populate it with all the edges for each point. 

$A$ is a **weighted** matrix so it is populated with the distance between each point (we are using Euclidean distance but this can be any distance metric). If this was unweighted we'd populate each edge with 1 to just highlight that an edge exists

**Example**

In the snapshot below of $A$, Image 2 and Image 0 have an edge. So we populate entry $[0,2]$ and $[2,0]$ with the distance between Image 0 and Image 2. We populate both because if Image 0 is connected to Image 2 then Image 2 must be connected to  Image 0. Hence $A$ is **symmetrical**

(What we are doing is called K-ISOMAP. There is also $\epsilon$-ISOMAP where we instead of a point having an edge with its $k$ nearest neighbours, it has an edge with all points within some distance $\epsilon$)


```python
from scipy.spatial.distance import cdist
import numpy as np
import pandas as pd

num_neighbors=100

# Compute pairwise distances for all images
distances = cdist(XA=images, XB=images, metric='euclidean')

# Create adjcency matrix A. For each image calculate nearest k neighbours and populate A
A = np.zeros_like(distances)
for i in range(distances.shape[0]):
    neighbors = np.argsort(distances[i,:])[:num_neighbors+1]
    A[i,neighbors] = distances[i,neighbors]
    A[neighbors,i] = distances[neighbors,i]

pd.DataFrame(A).head(10).to_markdown()
```




    |    |       0 |       1 |       2 |        3 |       4 |       5 |        6 |       7 |       8 |       9 |       10 |
    |---:|--------:|--------:|--------:|---------:|--------:|--------:|---------:|--------:|--------:|--------:|---------:|
    |  0 | 0       |  0      | 6.74324 |  0       |  0      |  0      |  0       |  0      |  0      |  0      |  0       |
    |  1 | 0       |  0      | 0       |  0       |  0      | 15.1985 |  0       |  0      | 11.8893 |  0      |  0       |
    |  2 | 6.74324 |  0      | 0       |  0       |  0      |  0      |  0       |  0      |  0      |  0      |  0       |
    |  3 | 0       |  0      | 0       |  0       |  0      | 15.1001 |  5.72698 | 14.1491 |  0      |  0      | 14.6427  |
    |  4 | 0       |  0      | 0       |  0       |  0      |  0      |  0       |  0      |  0      | 12.7192 |  0       |
    |  5 | 0       | 15.1985 | 0       | 15.1001  |  0      |  0      |  0       | 12.02   |  0      |  0      |  9.03674 |
    |  6 | 0       |  0      | 0       |  5.72698 |  0      |  0      |  0       | 15.6581 |  0      |  0      | 16.5017  |
    |  7 | 0       |  0      | 0       | 14.1491  |  0      | 12.02   | 15.6581  |  0      |  0      |  0      |  8.70689 |
    |  8 | 0       | 11.8893 | 0       |  0       |  0      |  0      |  0       |  0      |  0      |  0      |  0       |
    |  9 | 0       |  0      | 0       |  0       | 12.7192 |  0      |  0       |  0      |  0      |  0      |  0       |



### Distance Matrix
Now we want to create our geodesic matrix $D$ which calculates the shortest path between all points by "jumping" from point to point.

We'll use Dijkstra's algorithm to compute shortest distances


```python
from sklearn.utils.graph import graph_shortest_path

# Matrix of shortest distances between all points
D = graph_shortest_path(A, method='D')

pd.DataFrame(D).head(10).to_markdown()
```




    |    |        0 |       1 |        2 |        3 |       4 |       5 |        6 |       7 |       8 |       9 |       10 |
    |---:|---------:|--------:|---------:|---------:|--------:|--------:|---------:|--------:|--------:|--------:|---------:|
    |  0 |  0       | 35.3093 |  6.74324 | 35.9519  | 22.4629 | 36.3797 | 38.7622  | 28.5315 | 40.5984 | 21.4306 | 30.3151  |
    |  1 | 35.3093  |  0      | 29.559   | 22.9185  | 40.1024 | 15.1985 | 25.6406  | 22.2236 | 11.8893 | 29.59   | 22.5981  |
    |  2 |  6.74324 | 29.559  |  0       | 35.9062  | 23.0037 | 29.6364 | 38.7164  | 27.3635 | 40.2617 | 22.0929 | 29.2125  |
    |  3 | 35.9519  | 22.9185 | 35.9062  |  0       | 36.9273 | 15.1001 |  5.72698 | 14.1491 | 24.4756 | 29.3544 | 14.6427  |
    |  4 | 22.4629  | 40.1024 | 23.0037  | 36.9273  |  0      | 41.3847 | 34.4033  | 40.2385 | 32.486  | 12.7192 | 44.3224  |
    |  5 | 36.3797  | 15.1985 | 29.6364  | 15.1001  | 41.3847 |  0      | 20.827   | 12.02   | 20.7669 | 30.1574 |  9.03674 |
    |  6 | 38.7622  | 25.6406 | 38.7164  |  5.72698 | 34.4033 | 20.827  |  0       | 15.6581 | 27.6055 | 30.3028 | 16.5017  |
    |  7 | 28.5315  | 22.2236 | 27.3635  | 14.1491  | 40.2385 | 12.02   | 15.6581  |  0      | 27.5928 | 37.0327 |  8.70689 |
    |  8 | 40.5984  | 11.8893 | 40.2617  | 24.4756  | 32.486  | 20.7669 | 27.6055  | 27.5928 |  0      | 25.2031 | 25.7615  |
    |  9 | 21.4306  | 29.59   | 22.0929  | 29.3544  | 12.7192 | 30.1574 | 30.3028  | 37.0327 | 25.2031 |  0      | 38.8282  |



### Centered Matrix
We have a matrix of shortest distances now we want to center it. Our matrix $C$ is calculated as $$C = 1 - \frac{1}{2m} H(D^2)H$$

Where $H = I - \frac{1}{m}11^T$, here $I$ is the identity matrix and $1$ is just a vector of 1s (of length $m$). 

The theory behind centering matrices is beyond the scope of this post but remember **this is just the matrix equivalent of subtracting the mean from each point!**




```python
m = D.shape[0]

# Compute the centering matrix H:
I = np.eye(N=D.shape[1])
H = I - 1/m * np.ones(I.shape) * np.ones(I.shape).T

# Compute the C matrix:
C = -1/(2*m) * H @ D**2 @ H

pd.DataFrame(C).head(10).to_markdown()
```




    |    |          0 |         1 |          2 |         3 |          4 |         5 |          6 |          7 |          8 |          9 |        10 |       
    |---:|-----------:|----------:|-----------:|----------:|-----------:|----------:|-----------:|-----------:|-----------:|-----------:|----------:|
    |  0 |  0.391491  | -0.43338  |  0.327678  | -0.423125 |  0.189568  | -0.454497 | -0.466608  | -0.0940665 | -0.597125  |  0.0471495 | -0.124533 |
    |  1 | -0.43338   |  0.52792  | -0.197418  |  0.19472  | -0.532779  |  0.3963   |  0.206953  |  0.203489  |  0.55051   | -0.182843  |  0.236183 |
    |  2 |  0.327678  | -0.197418 |  0.329011  | -0.452012 |  0.140713  | -0.166853 | -0.49531   | -0.0785396 | -0.608866  | -0.0047385 | -0.108754 |
    |  3 | -0.423125  |  0.19472  | -0.452012  |  0.614036 | -0.314522  |  0.441495 |  0.697464  |  0.456926  |  0.265704  | -0.129837  |  0.491465 |
    |  4 |  0.189568  | -0.532779 |  0.140713  | -0.314522 |  0.71054   | -0.573777 | -0.0786338 | -0.511257  | -0.0128948 |  0.419777  | -0.713913 |
    |  5 | -0.454497  |  0.3963   | -0.166853  |  0.441495 | -0.573777  |  0.595617 |  0.401028  |  0.487628  |  0.376688  | -0.17328   |  0.577346 |
    |  6 | -0.466608  |  0.206953 | -0.49531   |  0.697464 | -0.0786338 |  0.401028 |  0.82788   |  0.531627  |  0.255856  | -0.0634463 |  0.556913 |
    |  7 | -0.0940665 |  0.203489 | -0.0785396 |  0.456926 | -0.511257  |  0.487628 |  0.531627  |  0.58663   |  0.135734  | -0.508682  |  0.577045 |
    |  8 | -0.597125  |  0.55051  | -0.608866  |  0.265704 | -0.0128948 |  0.376688 |  0.255856  |  0.135734  |  0.775615  |  0.113192  |  0.250447 |
    |  9 |  0.0471495 | -0.182843 | -0.0047385 | -0.129837 |  0.419777  | -0.17328  | -0.0634463 | -0.508682  |  0.113192  |  0.360789  | -0.561533 |



### Calculate eigenvectors

We've got our centered, symmetrical matrix of shortest distances. Now we can run eigen-decomposition. Then we take the first $k$ eigenvectors depending on how many components we want and scale each of them by their corresponding eigenvalue

In our case we'll take the first 2 eigenvectors


```python
dimensions =2

# Compute the eigenvalues and eigenvectors of C
eigenvalues, eigenvectors = np.linalg.eig(C)

# Normalize the leading eigenvectors:
Z = eigenvectors[:,:dimensions] * np.sqrt(eigenvalues[:dimensions])

# Show results
pd.DataFrame(Z).head(10).to_markdown()
```



    |    |          0 |          1 |
    |---:|-----------:|-----------:|
    |  0 | -0.553615  |  0.150386  |
    |  1 |  0.514445  | -0.215476  |
    |  2 | -0.574467  |  0.0201345 |
    |  3 |  0.552808  | -0.398368  |
    |  4 | -0.169181  |  0.618504  |
    |  5 |  0.5209    | -0.465865  |
    |  6 |  0.613496  | -0.510007  |
    |  7 |  0.25741   | -0.758015  |
    |  8 |  0.812778  |  0.206535  |
    |  9 |  0.0186157 |  0.546089  |



### Results
Below we have our results plotted with a sample of images overlayed. We can see that right-facing images are clustered together on the right hand side and left-facing images are on the left side. Images facing down are at the bottom and images facing up are at the top. 

ISOMAP has done a good job at identifying similar images in a low-dimensional space!


```python
import math

def plot_image_projection(projection_data, isomap_data, sample_size):
    
    df = pd.DataFrame(projection_data)
    num_images, num_pixels = isomap_data.shape

    f,ax = plt.subplots(figsize=(10,10))

    # Show 40 of the images ont the plot
    x_size = (max(df[0]) - min(df[0])) * 0.05
    y_size = (max(df[1]) - min(df[1])) * 0.05

    for i in range(sample_size):
        img_num = np.random.randint(0, num_images)
        x0 = df.loc[img_num, 0] - (x_size / 2.)
        y0 = df.loc[img_num, 1] - (y_size / 2.)
        x1 = df.loc[img_num, 0] + (x_size / 2.)
        y1 = df.loc[img_num, 1] + (y_size / 2.)

        img = pd.DataFrame(isomap_data).iloc[img_num,:].values.reshape(int(math.sqrt(num_pixels))
                                                                  , int(math.sqrt(num_pixels))).T

        ax.imshow(img
                  , aspect='auto'
                  , cmap=plt.cm.gray
                  , interpolation='nearest'
                  , zorder=100000
                  , extent=(x0, x1, y0, y1))

    ax.scatter(df[0], df[1], marker='.' ,alpha=0.7)

    ax.set_ylabel('Up-Down Pose')
    ax.set_xlabel('Right-Left Pose')

# Plot results
plot_image_projection(Z, images, 100)
plt.title('ISOMAP Results', fontsize=20);
```

![png](../../images/isomap/output_13_0.png)



### Comparing against PCA
Running PCA on these images we can immediately see the differences. On the left side we have a mixture of left and right facing images and above/below images. At the top we have some right facing images from below and more straight on images from above.. not ideal




```python
from sklearn.decomposition import PCA

# Calculate PCA components
pca_comp = PCA(n_components=dim).fit_transform(images)

# Plot
plot_image_projection(pca_comp, images, 100)
plt.title('PCA Results', fontsize=20);
```

![_config.yml]({{ site.baseurl }}/images/isomap/output_15_0.png)


### Code
Below is the ISOMAP code we've gone through as a function if you want to implement it yourself


```python
from sklearn.utils.graph import graph_shortest_path
import numpy as np
from scipy.spatial.distance import cdist

def ISOMAP(data, n_neighbours, dimensions, distance_metric):
    
    # Number of data points
    m = data.shape[0]
    
    # Compute pairwise distances for all images
    distances = cdist(XA=data, XB=data, metric=distance_metric)
    
    # Adjacency matrix
    A = np.zeros_like(distances)
    for i in range(distances.shape[0]):
        # Index of closest nearest k-neighbours
        neighbours = np.argsort(distances[i,:])[:n_neighbours+1]

        # Fill A with relevant distances
        A[i,neighbours] = distances[i,neighbours]
        A[neighbours,i] = distances[neighbours,i]
    
    # Matrix of shortest distances between all points
    D = graph_shortest_path(A, method='D')
    
    # Compute the centering matrix H:
    I = np.eye(N=D.shape[1])
    H = I - 1/m * np.ones(I.shape) * np.ones(I.shape).T

    # Compute the C matrix:
    C = -1/(2*m) * H @ D**2 @ H
    
    # Eigen-decomposition on C
    eigenvalues, eigenvectors = np.linalg.eig(C)
    
    # Generate projection of data on to eigenvectors
    Z = eigenvectors[:,:dimensions] * np.sqrt(eigenvalues[:dimensions])
    
    return Z
```

### Conclusion
In this post we've gone through how to build your own ISOMAP algorithm and shown an example of when to use non-linear dimensionality reduction over linear dimensionality reduction

**Thanks for reading!**
